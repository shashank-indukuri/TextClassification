{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashank-indukuri/TextClassification/blob/main/custom_ner_using_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wXLDL-w9I9AM"
      },
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "metadata": {
        "id": "zjBMLlGDI9AU"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "In this notebook we will be learning about how to set custom named entity recognition (NER) in spacy.\n",
        "\n",
        "* **What is Named Entity**?\n",
        "\n",
        "A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. \n",
        "\n",
        "\n",
        "* **About spaCy** :-\n",
        "\n",
        "spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. The library is published under the MIT license and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion. [More](https://spacy.io/)\n",
        "\n",
        "* **NLTK vs spaCy** :-\n",
        "\n",
        "While NLTK provides access to many algorithms to get something done, spaCy provides the best way to do it. It provides the fastest and most accurate syntactic analysis of any NLP library released to date. It also offers access to larger word vectors that are easier to customize. For an app builder mindset that prioritizes getting features done, spaCy would be the better choice. [More](https://www.activestate.com/blog/natural-language-processing-nltk-vs-spacy/#:~:text=While%20NLTK%20provides%20access%20to,that%20are%20easier%20to%20customize.)\n",
        "\n",
        "* **Some [Features](https://spacy.io/usage/spacy-101#features) of spaCy** :- \n",
        "\n",
        "    * Tokenization\n",
        "    * Part-of-speech (POS) Tagging\n",
        "    * Lemmatization\n",
        "    * Named Entity Recognition (NER)\n",
        "    * Similarity\n",
        "    * Text Classification\n",
        "\n",
        "We will be focusing on NER.\n",
        "\n",
        "* **What is NER ?**\n",
        "\n",
        "Named-entity recognition is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. [More](https://en.wikipedia.org/wiki/Named-entity_recognition)"
      ]
    },
    {
      "metadata": {
        "id": "VC13KwT1I9AX"
      },
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "metadata": {
        "id": "EmCWtb_kI9AY"
      },
      "cell_type": "markdown",
      "source": [
        "Above NER example shows all named entities which are present in given sentence.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "o3eblULtI9AY"
      },
      "cell_type": "markdown",
      "source": [
        "**Myself [Amar Sharma](https://www.kaggle.com/amarsharma768)  and my team member [Parvez Shaikh](https://www.kaggle.com/parvezahmedshaikh) created many such notebooks as part of the course work under \"Master in Data Science Programme\" at [Suven](https://datascience.suvenconsultants.com/) , under mentor-ship of [Rocky Jagtiani](https://www.linkedin.com/in/rocky-jagtiani-3b390649/) .**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QzcN5sduI9AZ"
      },
      "cell_type": "markdown",
      "source": [
        "# **Coding part**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fFh7xk4xI9Aa"
      },
      "cell_type": "code",
      "source": [
        "# pip install spacy\n",
        "\n",
        "# Install spacy if not already installed."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "c_kC0PCmI9Ad"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pickle\n",
        "import random\n",
        "# Import required modules"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HjcfBieeI9Ae"
      },
      "cell_type": "markdown",
      "source": [
        "First, we will execute spacy model on a simple sentence just to get familiar with it."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3J63OJnI9Af",
        "outputId": "c430e344-b65a-404d-9b61-bfbc5e77307d"
      },
      "cell_type": "code",
      "source": [
        "test = spacy.load('en_core_web_sm')\n",
        "sent = '''My name is Amar Sharma, i stay in Mumbai.\n",
        "The 2020 america presidential election is scheduled for Tuesday, November 3.'''\n",
        "\n",
        "ts = test(sent)\n",
        "for ent in ts.ents:\n",
        "  print(f'{ent.label_.upper():{10}} - {ent.text}')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PERSON     - Amar Sharma\n",
            "GPE        - Mumbai\n",
            "DATE       - 2020\n",
            "DATE       - Tuesday, November 3\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "6rKK1ghdI9Ag"
      },
      "cell_type": "markdown",
      "source": [
        "## **Explanation of above code** :-\n",
        "\n",
        "**test = spacy.load('en')**, here **load()** is used to load model, '**en**' specifies name/unicode of model to load i.e., English. [More](https://spacy.io/api/top-level#spacy.load)\n",
        "\n",
        "**sent** contains sentence passed for extracting named entities.\n",
        "\n",
        "**ts = test(sent)**, here we pass sent variable to test (object of spacy english model) which learns POS tags,NER etc and stores information in ts.\n",
        "\n",
        "**ts.ents** Here ents property contains all the entities as tuple identified by our model from sents.\n",
        "\n",
        "**ent.label_** contains label**(PERSON / GPE / DATE)** which is given by model to that entitiy.\n",
        "\n",
        "**ent.text** contains entity**(Amar Sharma / Mumbai / 2020 / america)** as string.\n",
        "\n",
        "\n",
        "Have you noticed that even as we specified america word initial character 'a' as smaller case still our model managed to identiy it as GPE."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTF1sPhHI9Ah",
        "outputId": "adffdc64-c5d2-42af-c9d4-66070dad8e30"
      },
      "cell_type": "code",
      "source": [
        "# one more example\n",
        "\n",
        "for ent in test(\"Apple is looking at buying U.K. startup for $1 billion\").ents:\n",
        "  print(f'{ent.label_.upper():{10}} - {ent.text}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORG        - Apple\n",
            "GPE        - U.K.\n",
            "MONEY      - $1 billion\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "BXvjYYZ9I9Ai"
      },
      "cell_type": "markdown",
      "source": [
        "By the way, we can use explain method of spacy to know in what a named entites signifies. \n",
        "\n",
        "See below code."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgYdubmuI9Aj",
        "outputId": "22eff138-64bc-42d1-d92b-839743c484be"
      },
      "cell_type": "code",
      "source": [
        "print(f'PERSON - {spacy.explain(\"PERSON\")}')\n",
        "print(f'GPE    - {spacy.explain(\"GPE\")}')\n",
        "print(f'DATE   - {spacy.explain(\"DATE\")}')\n",
        "print(f'MONEY  - {spacy.explain(\"MONEY\")}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PERSON - People, including fictional\n",
            "GPE    - Countries, cities, states\n",
            "DATE   - Absolute or relative dates or periods\n",
            "MONEY  - Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "IX66ZIVGI9Ak"
      },
      "cell_type": "markdown",
      "source": [
        "[More](https://spacy.io/api/annotation#named-entities) predefined named entites present in spacy."
      ]
    },
    {
      "metadata": {
        "id": "9_Wn58DaI9Ak"
      },
      "cell_type": "markdown",
      "source": [
        "# **Why we are creating custom NER ?**"
      ]
    },
    {
      "metadata": {
        "id": "E8bynl2WI9Ak"
      },
      "cell_type": "markdown",
      "source": [
        "Now, if spacy has so many named entities predefined then why are we creating custom NER.\n",
        "Well, in simple words we never trained our model it was pre trained, we directly created object of it and started extracting information on it which our model did very well. But, what happens when we try to get all named entities on a dataset that it was never trained  for. We are going to try one such example."
      ]
    },
    {
      "metadata": {
        "id": "WBebKrEEI9Ak"
      },
      "cell_type": "markdown",
      "source": [
        "We will extract data from a resume and use this data as test data for our model to predict named entities from."
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "-sN9ZWXrI9Al",
        "outputId": "5c4d920c-1c95-4363-c42d-8c8a34220ce3"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/kaggle/input/dataset-for-resume-information-retrieval/Alice Clark CV.pdf\n/kaggle/input/dataset-for-resume-information-retrieval/train_data.pkl\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yT7MKuqPI9Al"
      },
      "cell_type": "code",
      "source": [
        "# since our resume is in pdf format we will use PyMuPDF to extract data from it.\n",
        "# You can also use PyPDF2."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAb5TqnpI9Al",
        "outputId": "d6e337a0-89bb-4b12-9a1e-1be3a78275df"
      },
      "cell_type": "code",
      "source": [
        "pip install PyMuPDF # https://pypi.org/project/PyMuPDF/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.22.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.22.3\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "_MW0jUOFI9Am",
        "outputId": "0919b2a1-568a-4be8-d539-aa9cd1cd9242"
      },
      "cell_type": "code",
      "source": [
        "import sys,fitz\n",
        "fname = 'Alice Clark CV.pdf'\n",
        "doc= fitz.open(fname)\n",
        "alice_cv=\"\"\n",
        "for page in doc:\n",
        "  alice_cv = alice_cv + str(page.getText())\n",
        "\n",
        "print(alice_cv)\n",
        "\n",
        "# we have extracted the data from pdf file using PyMuPDF and stored in alice_cv variable."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b7f322897a0b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0malice_cv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0malice_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malice_cv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malice_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Page' object has no attribute 'getText'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "iPmZ4wNaI9An"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can pass our test data on our pre trained spacy model and evaluate how good it has performed."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "KkoTL_jNI9Ao"
      },
      "cell_type": "code",
      "source": [
        "test = spacy.load('en_core_web_sm')\n",
        "ts = test(\"Hcid search for 123XBV678\") # we have splitted our data with '\\n' and rejoined with space. "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbP02oXeI9Ap"
      },
      "cell_type": "markdown",
      "source": [
        "**ts** variable contains all POS tags,NER etc in it. We will extract only NER and manually verify output."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ts.ents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3Qfq6Z9LGQW",
        "outputId": "2fd124ea-7248-44e0-8c12-ee01a57e8645"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Hcid,)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xowSaLelI9Ap"
      },
      "cell_type": "code",
      "source": [
        "# Here, we are only extracting all PERSON named entities.\n",
        "\n",
        "for ent in ts.ents:\n",
        "  if ent.label_.upper() == 'PERSON':\n",
        "    print(f'{ent.label_.upper():{10}} - {ent.text}')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hd6Ji2yAI9Ap"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see our pre trained model did not perform well on test data. Only name('Alice Clark') was labelled correctly and rest all are labelled incorrect."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9SOfL9yCI9Aq",
        "outputId": "c3a61c67-4034-41a9-d30a-0dbdbb3cebc0"
      },
      "cell_type": "code",
      "source": [
        "# Here, we are only extracting all ORG named entities.\n",
        "\n",
        "for ent in ts.ents:\n",
        "  if ent.label_.upper() == 'ORG':\n",
        "    print(f'{ent.label_.upper():{10}} - {ent.text}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "ORG        - AI / Machine Learning\nORG        - star/snow flake scema\nORG        - SQL\nORG        - Microsoft Azure\nORG        - SQL Azure\nORG        - Stream Analytics\nORG        - Power BI\nORG        - Power BI\nORG        - SQL\nORG        - Microsoft\nORG        - Microsoft\nORG        - Microsoft\nORG        - Microsoft\nORG        - Microsoft Edge\nORG        - Microsoft\nORG        - Microsoft\nORG        - PBI\nORG        - Technology/Tools\nORG        - Indian Institute of Technology\nORG        - Big Data\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PPxxHsAqI9Aq"
      },
      "cell_type": "markdown",
      "source": [
        "ORG named entities are also labelled incorrect except 'MICROSOFT'.\n",
        "\n",
        "Now, in these types of situation where labelling are almost incorrect what should we do?"
      ]
    },
    {
      "metadata": {
        "id": "wAg4teloI9Aq"
      },
      "cell_type": "markdown",
      "source": [
        "# Solution \n",
        "\n",
        "\n",
        "This is the reason why we have to first train our spacy model on some manually labelled data and create custom NER. \n",
        "\n",
        "Since, for testing we took resume data means we are here dealing with resume's, so we have to first train our spacy model on some manually labelled resume data. For training purpose we got data from online but you can create training data according to your requirement."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7aQWhyv3I9Aq",
        "outputId": "0b88f7ec-7484-48b8-df1a-3ba8f687b687"
      },
      "cell_type": "code",
      "source": [
        "train_data = pickle.load(open('/kaggle/input/dataset-for-resume-information-retrieval/train_data.pkl','rb'))\n",
        "print(f\"Training data consist of {len(train_data)} manually labelled resume's.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training data consist of 200 manually labelled resume's.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8mJq-vgoI9As"
      },
      "cell_type": "markdown",
      "source": [
        "We have our training data stored in pickle file."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "oBFt0vbcI9As",
        "outputId": "6f96355c-80e1-451e-db5b-5495a08b4b0c"
      },
      "cell_type": "code",
      "source": [
        "# Checking format of one resume data\n",
        "\n",
        "train_data[97]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "('Ramesh chokkala Telangana - Email me on Indeed: indeed.com/r/Ramesh-chokkala/16d5fa56f8c19eb6  WORK EXPERIENCE  software  Microsoft,Infosis, Google -  May 2018 to Present  software  Microsoft,Infosis, Google -  May 2018 to Present  EDUCATION  btech  Trinity engineering college  https://www.indeed.com/r/Ramesh-chokkala/16d5fa56f8c19eb6?isid=rex-download&ikw=download-top&co=IN',\n {'entities': [(250, 278, 'College Name'),\n   (243, 248, 'Degree'),\n   (182, 207, 'Companies worked at'),\n   (172, 180, 'Designation'),\n   (122, 147, 'Companies worked at'),\n   (112, 120, 'Designation'),\n   (48, 94, 'Email Address'),\n   (16, 25, 'Location'),\n   (0, 15, 'Name')]})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "xZkMnb1QI9At"
      },
      "cell_type": "markdown",
      "source": [
        "### Anatomy of our train data\n",
        "\n",
        "Our train data is stored as a tuple consisting of 200 resume data, each resume data consist of 2 parts/indexes.\n",
        "\n",
        "* First index  [0] consist of all details(name, degree, designation, compaines worked at) in resume.\n",
        "* Second index [1] consist of a dictionary object having only one key i.e., 'entities' and look carefully at its value.\n",
        "\n",
        "Value of 'entities' key has a list of tuples and in each tuple we have some number and some labelling. \n",
        "\n",
        "For Eg :- **(0, 15, 'Name')**, here 0 denotes start index and 15 denotes end index of label 'Name', which is 'Ramesh chokkala'.\n",
        "Similarly, we can see that all the other tuple also has some start and end index alongwith their respective label. This is how you can manually create data for training.\n",
        "\n",
        "**Note** :- label of all training data should be same i.e., if you have specified label as 'Name' for one resume then for all the resume data wherever name is present for that label should be as 'Name' only and not something else."
      ]
    },
    {
      "metadata": {
        "id": "N7_xbiwlI9Au"
      },
      "cell_type": "markdown",
      "source": [
        "As we have our training data ready, we will now train our spacy model and add custom NER."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "0CfDQ6w7I9Au"
      },
      "cell_type": "code",
      "source": [
        "# loading blank spacy model as we want to customize our model.\n",
        "# spacy.blank('en') will create a blank model of a given language class i.e., for here English.\n",
        "\n",
        "nlp = spacy.blank('en') "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rE7mv-dXI9Au"
      },
      "cell_type": "markdown",
      "source": [
        "### Spacy pipeline\n",
        "\n",
        "Below is the pipeline which is created when we pass our data to spacy model. \n",
        "\n",
        "Recommended [More](https://spacy.io/usage/processing-pipelines)."
      ]
    },
    {
      "metadata": {
        "id": "f8gdV7daI9Av"
      },
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "S8up3tEyI9Av"
      },
      "cell_type": "code",
      "source": [
        "# Creating a function to train our model\n",
        "\n",
        "def train_model(train_data):\n",
        "    \n",
        "  if 'ner' not in nlp.pipe_names:# Checking if NER is present in pipeline\n",
        "    ner = nlp.create_pipe('ner') # creating NER pipe if not present\n",
        "    nlp.add_pipe(ner, last=True) # adding NER pipe in the end\n",
        "\n",
        "  for _, annotation in train_data: # Getting 1 resume at a time from our training data of 200 resumes\n",
        "    for ent in annotation['entities']: # Getting each tuple at a time from 'entities' key in dictionary at index[1] i.e.,(0, 15, 'Name') and so on\n",
        "      ner.add_label(ent[2])  # here we are adding only labels of each tuple from entities key dict, eg:- 'Name' label of (0, 15, 'Name')\n",
        "    \n",
        "  # In above for loop we finally added all custom NER from training data.\n",
        "    \n",
        "\n",
        "  other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner'] # getting all other pipes except NER.\n",
        "  with nlp.disable_pipes(*other_pipes): # Disabling other pipe's as we want to train only NER.\n",
        "        optimizer = nlp.begin_training()\n",
        "        \n",
        "        for itn in range(10):         # trainig model for 10 iteraion\n",
        "            print('Starting iteration ' + str(itn))\n",
        "            random.shuffle(train_data) # shuffling data in every iteration \n",
        "            losses = {}\n",
        "            for text, annotations in train_data:\n",
        "              try:\n",
        "                nlp.update(\n",
        "                    [text],        #batch of texts\n",
        "                    [annotations], #batch of annotations\n",
        "                    drop=0.2,      #dropout rate -makes it harder to memorise\n",
        "                    sgd=optimizer, #callable to update weights\n",
        "                    losses=losses) #Dictionary to update with the loss, keyed by pipeline component.\n",
        "              except Exception as e:\n",
        "                pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aU6k9ec2I9A6"
      },
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "metadata": {
        "id": "WnqkUCxLI9A7"
      },
      "cell_type": "markdown",
      "source": [
        "We call our above created funcion **'train_model'** to learn from training data."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "aQbUFXVxI9A-",
        "outputId": "0a064b9b-e0ee-4a8b-9801-21e0d00fdb63"
      },
      "cell_type": "code",
      "source": [
        "# pass train data to function.\n",
        "\n",
        "train_model(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Starting iteration 0\nStarting iteration 1\nStarting iteration 2\nStarting iteration 3\nStarting iteration 4\nStarting iteration 5\nStarting iteration 6\nStarting iteration 7\nStarting iteration 8\nStarting iteration 9\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "N12fI0u2I9A_"
      },
      "cell_type": "code",
      "source": [
        "# Saving our trained model to re-use.\n",
        "\n",
        "nlp.to_disk('nlp_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "R9GkPDP4I9A_"
      },
      "cell_type": "code",
      "source": [
        "# Loading our trained model\n",
        "\n",
        "nlp_model = spacy.load('nlp_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "MEDe8wykI9A_",
        "outputId": "09ff4b3a-6a3f-4e45-e165-4aa1bd9a5870"
      },
      "cell_type": "code",
      "source": [
        "# Checking all the custom NER created\n",
        "\n",
        "nlp_model.entity.labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "('College Name',\n 'Companies worked at',\n 'Degree',\n 'Designation',\n 'Email Address',\n 'Graduation Year',\n 'Location',\n 'Name',\n 'Skills',\n 'UNKNOWN',\n 'Years of Experience')"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HZZPzzpWI9BA",
        "outputId": "41a6c719-d6fe-4d3b-9644-6189e54981ac"
      },
      "cell_type": "code",
      "source": [
        "doc = nlp_model(\" \".join(alice_cv.split('\\n')))\n",
        "for ent in doc.ents:\n",
        "  print(f'{ent.label_.upper():{20}} - {ent.text}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "NAME                 - Alice Clark\nDESIGNATION          - AI / Machine Learning\nLOCATION             - Delhi\nDEGREE               - Database: Experience in database designing, scalability, back-up and recovery, writing and\nDESIGNATION          - Software Engineer\nCOMPANIES WORKED AT  - Microsoft –\nCOLLEGE NAME         - Indian Institute of Technology – Mumbai\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YfyrrCgwI9BA"
      },
      "cell_type": "markdown",
      "source": [
        "From above output we can clearly see that our custom trained spacy model has worked very well and labelled our testing data correctly but not 100% as the skills are not mentioned in output. This could be due to less training data. To increase accuracy we should train our model on different formats of data."
      ]
    },
    {
      "metadata": {
        "id": "llYzTetdI9BA"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "As per our requirement, we can manually create data for training, add custom NER and train our model."
      ]
    },
    {
      "metadata": {
        "id": "WEzxWAuXI9BB"
      },
      "cell_type": "markdown",
      "source": [
        "# References \n",
        "\n",
        "* https://spacy.io/\n",
        "* https://github.com/laxmimerit/Resume-and-CV-Summarization-and-Parsing-with-Spacy-in-Python"
      ]
    },
    {
      "metadata": {
        "id": "igmoVdWLI9BB"
      },
      "cell_type": "markdown",
      "source": [
        "I would like to humbly and sincerely thank my mentor [Rocky Jagtiani](https://www.linkedin.com/in/rocky-jagtiani-3b390649/). He is more of a friend to me then mentor. The Python for Data Science taught by him and various assignments we did and are still doing is the best way to learn and skill in Data Science field.\n",
        "\n",
        "Recommended https://datascience.suvenconsultants.com/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}